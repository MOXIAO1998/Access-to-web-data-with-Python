# To run this, you can install BeautifulSoup
# https://pypi.python.org/pypi/beautifulsoup4

# Or download the file
# http://www.py4e.com/code3/bs4.zip
# and unzip it in the same directory as the file 

import urllib.request,urllib.parse,urllib.error
from bs4 import BeautifulSoup
import ssl #HTTPS

# Ignore SSl certificate errors
ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

url = input('Enter - ')
html = urllib.request.urlopen(url, context=ctx).read()
soup = BeautifulSoup(html, 'html.parser')

#Retrieve all of the achor tags
tags = soup('a')
for tag in tags:
  print(tag.get('href', None))

####################################################### Another Example ##################################################################
#Sample data: http://py4e-data.dr-chuck.net/comments_42.html (Sum=2553)
#Actual data: http://py4e-data.dr-chuck.net/comments_1104904.html (Sum ends with 40)

from urllib.request import urlopen
from bs4 import BeautifulSoup
import ssl

# Ignore SSL certificate errors
ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

url = input('Enter - ')
html = urlopen(url, context=ctx).read()
soup = BeautifulSoup(html, "html.parser")

# Retrieve all of the anchor tags
tags = soup('span')
for tag in tags:
    # Look at the parts of a tag
    print('TAG:', tag)
    print('URL:', tag.get('href', None))
    print('Contents:', tag.contents[0])
    print('Attrs:', tag.attrs)
######################################################## Another Example ##################################################################
#Sample problem: Start at http://py4e-data.dr-chuck.net/known_by_Fikret.html
#Find the link at position 3 (the first name is 1). Follow that link. Repeat this process 4 times. The answer is the last name that you retrieve.
#Sequence of names: Fikret Montgomery Mhairade Butchi Anayah
#Last name in sequence: Anayah

#Actual problem: Start at: http://py4e-data.dr-chuck.net/known_by_Ryleigh.html
#Find the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve.
#Hint: The first character of the name of the last page that you will load is: R

import urllib.request, urllib.parse, urllib.error
from bs4 import BeautifulSoup
import ssl

# Ignore SSL certificate errors
ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

url = input('Enter - ')
count = int(input('Enter counts:'))
position = int(input('Enter position'))-1
html = urllib.request.urlopen(url, context=ctx).read()
soup = BeautifulSoup(html, 'html.parser')

# Retrieve all of the anchor tags
Sequence = []
tags = soup('a')
for i in range(count):                                              # (0-6)
    link = tags[position].get('href', None)                         # pick the required 18th link in the original/18th of original link/ 18th of 18th of the original link
    print("Retrieving:",link)                                       # print the 18th link
    Sequence.append(tags[position].contents[0])                     # append the name in the Sequence.list
    html = urllib.request.urlopen(link, context=ctx).read()         # access the 18th link of original link/ 18th link of 18th link of original link
    soup = BeautifulSoup(html, 'html.parser')                       # parse
    tags = soup('a')                                                # all links in the page
    link = tags[position].get('href', None)                         # pick the 18th link of the 18th link
print(Sequence[-1])
